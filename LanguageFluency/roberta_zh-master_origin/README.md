使用：中文预训练RoBERTa模型 
-------------------------------------------------
RoBERTa是BERT的改进版，通过改进训练任务和数据生成方式、训练更久、使用更大批次、使用更多数据等获得了State of The Art的效果；可以用Bert直接加载。


##  缺失文件
位于 **/stcvm-i10/D/t-dach/LanguageFluency/roberta_zh-master/** 

chinese_roberta_wwm_ext_L-12_H-768_A-12：存放RoBERTa预训练模型

data：存放待训练的数据，分别四部分，train1234.tsv

output：存放Roberta训练好的模型参数

